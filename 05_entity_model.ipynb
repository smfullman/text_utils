{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, load_model\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1000 sequences of length 20 randomly alternating between a 1 and a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex_data = np.random.randint(low=1, high=3., size=(1000,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2,\n",
       "       2, 1, 2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_data[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sequence, label it in one of three classes based on the number of consecutive 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_sequence(sequence):\n",
    "    \"\"\"\n",
    "    Read through each sequence and label it class [0,1,0] if the sequence has two consecutive 1's, class [0,0,1]\n",
    "    if it has three consecutive 1's, and class [0,0,1] otherwise.\n",
    "    \"\"\"\n",
    "    labels = list()\n",
    "    previous_i = 0\n",
    "    previous_two_i = 0\n",
    "    for i in sequence:\n",
    "        if i == 1 and previous_i == 1 and previous_two_i == 1:\n",
    "            labels.append(np.array([0,0,1]))\n",
    "        elif i == 1 and previous_i == 1:\n",
    "            labels.append(np.array([0,1,0]))\n",
    "        else:\n",
    "            labels.append(np.array([1,0,0]))\n",
    "        previous_two_i = previous_i\n",
    "        previous_i = i\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_data_labels = np.array([label_sequence(ex_data[i,:]) for i in range(ex_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_data_labels[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = ex_data.shape[1]\n",
    "HIDDEN_UNITS = 4\n",
    "N_SYMBOLS = 2\n",
    "EMBED_SIZE = 1\n",
    "NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_data = Input(shape=(MAX_LEN, ))\n",
    "    x = Embedding(input_dim=N_SYMBOLS, output_dim=EMBED_SIZE, input_length=MAX_LEN)(input_data)\n",
    "    x = LSTM(units=HIDDEN_UNITS, return_sequences=True)(x)\n",
    "    output = TimeDistributed(Dense(NUM_LABELS, activation='softmax'))(x)\n",
    "    \n",
    "    model = Model(inputs=[input_data], outputs=[output])\n",
    "    \n",
    "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 1)             2         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 4)             96        \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 3)             15        \n",
      "=================================================================\n",
      "Total params: 113\n",
      "Trainable params: 113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7624 - acc: 0.7574\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7589 - acc: 0.7574\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7554 - acc: 0.7574\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7519 - acc: 0.7574\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7481 - acc: 0.7574\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7437 - acc: 0.7574\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7385 - acc: 0.7574\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7323 - acc: 0.7574\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7244 - acc: 0.7574\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7145 - acc: 0.7574\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7022 - acc: 0.7574\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6875 - acc: 0.7574\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6701 - acc: 0.7574\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6506 - acc: 0.7574\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6296 - acc: 0.7574\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6080 - acc: 0.7574\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5862 - acc: 0.7574\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5640 - acc: 0.7574\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5414 - acc: 0.7574\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5184 - acc: 0.7574\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4951 - acc: 0.7583\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4715 - acc: 0.7699\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4474 - acc: 0.7828\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4231 - acc: 0.8003\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3991 - acc: 0.8319\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3754 - acc: 0.8589\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3530 - acc: 0.8643\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3317 - acc: 0.8771\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3118 - acc: 0.8941\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2933 - acc: 0.9208\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2764 - acc: 0.9445\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2621 - acc: 0.9589\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2490 - acc: 0.9621\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2363 - acc: 0.9641\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2238 - acc: 0.9678\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2116 - acc: 0.9734\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1996 - acc: 0.9747\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1880 - acc: 0.9750\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1768 - acc: 0.9750\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1662 - acc: 0.9750\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1573 - acc: 0.9750\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1494 - acc: 0.9750\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1420 - acc: 0.9750\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1353 - acc: 0.9750\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1291 - acc: 0.9750\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1234 - acc: 0.9750\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1182 - acc: 0.9750\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1135 - acc: 0.9750\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1091 - acc: 0.9750\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1050 - acc: 0.9750\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1011 - acc: 0.9750\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0975 - acc: 0.9750\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0941 - acc: 0.9750\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0909 - acc: 0.9750\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0879 - acc: 0.9750\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0850 - acc: 0.9750\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0823 - acc: 0.9750\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0797 - acc: 0.9750\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0772 - acc: 0.9750\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0748 - acc: 0.9750\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0725 - acc: 0.9750\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0703 - acc: 0.9750\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0682 - acc: 0.9750\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0661 - acc: 0.9750\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0641 - acc: 0.9750\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0622 - acc: 0.9750\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0603 - acc: 0.9750\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0584 - acc: 0.9750\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0566 - acc: 0.9750\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0549 - acc: 0.9750\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0532 - acc: 0.9778\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0515 - acc: 0.9836\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0498 - acc: 0.9836\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0483 - acc: 0.9836\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0468 - acc: 0.9836\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0454 - acc: 0.9871\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0400 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0375 - acc: 1.0000\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0363 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0351 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0340 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0329 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0307 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0288 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0270 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0244 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0237 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0215 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0208 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec6045f9e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=ex_data, y=ex_data_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2,\n",
       "        2, 2, 1, 2, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 2, 1, 1, 2, 2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_data[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.79882169e-01,   2.39737630e-01,   8.03801790e-02],\n",
       "        [  1.17834374e-01,   7.39907563e-01,   1.42258123e-01],\n",
       "        [  2.69769393e-02,   1.24302827e-01,   8.48720193e-01],\n",
       "        [  1.60899758e-02,   3.16763073e-02,   9.52233732e-01],\n",
       "        [  9.91118491e-01,   5.62047411e-04,   8.31940956e-03],\n",
       "        [  9.99995470e-01,   2.50739595e-06,   2.05002630e-06],\n",
       "        [  9.99987721e-01,   1.08899376e-05,   1.42583451e-06],\n",
       "        [  9.99981046e-01,   1.70224448e-05,   1.87679780e-06],\n",
       "        [  9.92020309e-01,   7.97460414e-03,   5.12205770e-06],\n",
       "        [  9.99899983e-01,   8.01646529e-05,   1.97743266e-05],\n",
       "        [  9.99993443e-01,   4.92478739e-06,   1.71398028e-06],\n",
       "        [  9.96875405e-01,   3.12279840e-03,   1.77940819e-06],\n",
       "        [  9.99870539e-01,   1.13919363e-04,   1.54884456e-05],\n",
       "        [  9.99992728e-01,   5.32445802e-06,   1.85208705e-06],\n",
       "        [  9.96808112e-01,   3.19010811e-03,   1.82513270e-06],\n",
       "        [  9.99872088e-01,   1.12369969e-04,   1.55402231e-05],\n",
       "        [  9.85263944e-01,   1.47225196e-02,   1.35195833e-05],\n",
       "        [  9.99921322e-01,   5.87870491e-05,   1.99276383e-05],\n",
       "        [  9.93757844e-01,   6.23743329e-03,   4.73713317e-06],\n",
       "        [  1.90653615e-02,   9.76625085e-01,   4.30953782e-03],\n",
       "        [  9.95837212e-01,   5.56321291e-04,   3.60650104e-03],\n",
       "        [  9.99994755e-01,   3.17210493e-06,   2.08447977e-06],\n",
       "        [  9.99987602e-01,   1.09719040e-05,   1.44998091e-06],\n",
       "        [  9.99981284e-01,   1.67598500e-05,   1.87321496e-06],\n",
       "        [  9.92155790e-01,   7.83921778e-03,   5.01861768e-06],\n",
       "        [  9.99899745e-01,   8.05656236e-05,   1.96872006e-05],\n",
       "        [  9.91663218e-01,   8.32999684e-03,   6.75441061e-06],\n",
       "        [  1.90083161e-02,   9.76408899e-01,   4.58280835e-03],\n",
       "        [  9.51885432e-03,   1.48051335e-02,   9.75676060e-01],\n",
       "        [  2.67665014e-02,   2.75094938e-02,   9.45724010e-01]],\n",
       "\n",
       "       [[  6.79882169e-01,   2.39737630e-01,   8.03801790e-02],\n",
       "        [  1.17834374e-01,   7.39907563e-01,   1.42258123e-01],\n",
       "        [  2.69769393e-02,   1.24302827e-01,   8.48720193e-01],\n",
       "        [  9.96340513e-01,   3.07404116e-04,   3.35212145e-03],\n",
       "        [  9.99996066e-01,   2.30515820e-06,   1.63557672e-06],\n",
       "        [  9.97754514e-01,   2.24422500e-03,   1.32916921e-06],\n",
       "        [  9.99867916e-01,   1.18932054e-04,   1.30730277e-05],\n",
       "        [  9.99992490e-01,   5.62321111e-06,   1.91016034e-06],\n",
       "        [  9.99985814e-01,   1.26342838e-05,   1.60786738e-06],\n",
       "        [  9.99982476e-01,   1.55565904e-05,   1.88601211e-06],\n",
       "        [  9.92814004e-01,   7.18154665e-03,   4.53035273e-06],\n",
       "        [  9.99897599e-01,   8.31133584e-05,   1.93392661e-05],\n",
       "        [  9.99993324e-01,   4.95694576e-06,   1.72690079e-06],\n",
       "        [  9.99984980e-01,   1.34979282e-05,   1.59415822e-06],\n",
       "        [  9.93077397e-01,   6.91837538e-03,   4.29735519e-06],\n",
       "        [  9.99894857e-01,   8.57533960e-05,   1.94434779e-05],\n",
       "        [  9.91020501e-01,   8.97208974e-03,   7.41140366e-06],\n",
       "        [  1.89967584e-02,   9.76354241e-01,   4.64898068e-03],\n",
       "        [  9.47777275e-03,   1.47896353e-02,   9.75732565e-01],\n",
       "        [  2.66626552e-02,   2.74506044e-02,   9.45886731e-01],\n",
       "        [  2.46499032e-02,   2.29427349e-02,   9.52407360e-01],\n",
       "        [  2.47439221e-02,   2.04656366e-02,   9.54790413e-01],\n",
       "        [  2.55346280e-02,   2.02926584e-02,   9.54172671e-01],\n",
       "        [  2.59976219e-02,   2.03171782e-02,   9.53685164e-01],\n",
       "        [  9.93720770e-01,   4.61536139e-04,   5.81778120e-03],\n",
       "        [  9.99995351e-01,   2.58667819e-06,   2.04668368e-06],\n",
       "        [  9.97729242e-01,   2.26946687e-03,   1.34918855e-06],\n",
       "        [  1.94878299e-02,   9.77078736e-01,   3.43334908e-03],\n",
       "        [  9.95555580e-01,   6.00041822e-04,   3.84443230e-03],\n",
       "        [  9.99994636e-01,   3.17047738e-06,   2.10813346e-06]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([ex_data[0:2,:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data here: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/entity-annotated-corpus/ner_dataset.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['word_lower'] = df['Word'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = list(set(df.word_lower.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_to_word = {i:w for i,w, in enumerate(word_vocab)}\n",
    "word_to_hash = {w:i for i,w in hash_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_hash'] = df['word_lower'].apply(lambda x: word_to_hash[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_label_dict = {'NN':np.array([1,0,0,0,0,0,0,0,0,0]),\n",
    "                  'NNP':np.array([0,1,0,0,0,0,0,0,0,0]),\n",
    "                  'IN':np.array([0,0,1,0,0,0,0,0,0,0]),\n",
    "                  'DT':np.array([0,0,0,1,0,0,0,0,0,0]),\n",
    "                  'JJ':np.array([0,0,0,0,1,0,0,0,0,0]),\n",
    "                  'NNS':np.array([0,0,0,0,0,1,0,0,0,0]),\n",
    "                  '.':np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "                  ',':np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "                  '``':np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "                  '$':np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "                  ':':np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "                  ';':np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "                  'VBD':np.array([0,0,0,0,0,0,0,1,0,0]),\n",
    "                  'VBN':np.array([0,0,0,0,0,0,0,0,1,0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_label'] = df.POS.apply(lambda x: pos_label_dict.get(x,np.array([0,0,0,0,0,0,0,0,0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048574"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLE_SIZE = 20000\n",
    "VAL_SAMPLE_SIZE = 1500\n",
    "TEST_SAMPLE_SIZE = 500\n",
    "SEQUENCE_LENGTH = 50\n",
    "VAL_START_INDEX = 850000\n",
    "TEST_INDEX_START = 900000\n",
    "N_CLASSES = 10\n",
    "\n",
    "def generate_train_val_test(df):\n",
    "    X_train = list()\n",
    "    y_train = list()\n",
    "    X_val = list()\n",
    "    y_val = list()\n",
    "    \n",
    "    for train_n in range(TRAIN_SAMPLE_SIZE):\n",
    "        random_start = np.random.randint(low=0, high=VAL_START_INDEX-SEQUENCE_LENGTH, size=1)[0]\n",
    "        X_train.append(np.array(df.loc[random_start:random_start+SEQUENCE_LENGTH-1]['word_hash'].tolist()))\n",
    "        y_train.append(np.vstack(df.loc[random_start:random_start+SEQUENCE_LENGTH-1]['pos_label'].tolist()))\n",
    "        \n",
    "    for val_n in range(VAL_SAMPLE_SIZE):\n",
    "        random_start = np.random.randint(low=VAL_START_INDEX, high=TEST_INDEX_START-SEQUENCE_LENGTH, size=1)[0]\n",
    "        X_val.append(np.array(df.loc[random_start:random_start+SEQUENCE_LENGTH-1]['word_hash'].tolist()))\n",
    "        y_val.append(np.vstack(df.loc[random_start:random_start+SEQUENCE_LENGTH-1]['pos_label'].tolist()))\n",
    "    \n",
    "    return np.vstack(X_train), np.reshape(np.vstack(y_train), (TRAIN_SAMPLE_SIZE,SEQUENCE_LENGTH,N_CLASSES)), np.vstack(X_val), np.reshape(np.vstack(y_val), (VAL_SAMPLE_SIZE,SEQUENCE_LENGTH,N_CLASSES)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = generate_train_val_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 50)\n",
      "(20000, 50, 10)\n",
      "(1500, 50)\n",
      "(1500, 50, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = X_train.shape[1]\n",
    "HIDDEN_UNITS = 32\n",
    "N_SYMBOLS = len(word_to_hash)\n",
    "EMBED_SIZE = 64\n",
    "NUM_LABELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_data = Input(shape=(MAX_LEN, ))\n",
    "    x = Embedding(input_dim=N_SYMBOLS, output_dim=EMBED_SIZE, input_length=MAX_LEN)(input_data)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Bidirectional(LSTM(units=HIDDEN_UNITS, return_sequences=True, kernel_regularizer=l2(l=0.001)))(x)\n",
    "    output = TimeDistributed(Dense(NUM_LABELS, activation='softmax'))(x)\n",
    "    \n",
    "    model = Model(inputs=[input_data], outputs=[output])\n",
    "    \n",
    "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 50, 64)            2036288   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 50, 64)            24832     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 50, 10)            650       \n",
      "=================================================================\n",
      "Total params: 2,061,770\n",
      "Trainable params: 2,061,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "FILE_PATH = \"models/ner_model_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(FILE_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 1500 samples\n",
      "Epoch 1/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.8072\n",
      "Epoch 00001: val_loss improved from inf to 0.17977, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.6553 - acc: 0.8074 - val_loss: 0.1798 - val_acc: 0.9485\n",
      "Epoch 2/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9638\n",
      "Epoch 00002: val_loss improved from 0.17977 to 0.13381, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.1284 - acc: 0.9638 - val_loss: 0.1338 - val_acc: 0.9570\n",
      "Epoch 3/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9724\n",
      "Epoch 00003: val_loss improved from 0.13381 to 0.12205, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0912 - acc: 0.9725 - val_loss: 0.1220 - val_acc: 0.9607\n",
      "Epoch 4/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9759\n",
      "Epoch 00004: val_loss improved from 0.12205 to 0.11871, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0764 - acc: 0.9759 - val_loss: 0.1187 - val_acc: 0.9612\n",
      "Epoch 5/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9785\n",
      "Epoch 00005: val_loss improved from 0.11871 to 0.11786, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0676 - acc: 0.9785 - val_loss: 0.1179 - val_acc: 0.9621\n",
      "Epoch 6/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9801\n",
      "Epoch 00006: val_loss improved from 0.11786 to 0.11736, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0616 - acc: 0.9801 - val_loss: 0.1174 - val_acc: 0.9625\n",
      "Epoch 7/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9813\n",
      "Epoch 00007: val_loss improved from 0.11736 to 0.11633, saving model to models/ner_model_weights.hdf5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0577 - acc: 0.9813 - val_loss: 0.1163 - val_acc: 0.9635\n",
      "Epoch 8/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9824\n",
      "Epoch 00008: val_loss did not improve\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0542 - acc: 0.9824 - val_loss: 0.1185 - val_acc: 0.9631\n",
      "Epoch 9/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9831\n",
      "Epoch 00009: val_loss did not improve\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0516 - acc: 0.9831 - val_loss: 0.1186 - val_acc: 0.9639\n",
      "Epoch 10/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9839\n",
      "Epoch 00010: val_loss did not improve\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.0491 - acc: 0.9839 - val_loss: 0.1205 - val_acc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febdeaa2860>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=EPOCHS, validation_data=[X_val, y_val], callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plain_text_to_sequence(text_string):\n",
    "    text_split = text_string.replace(\".\",\" .\").split()\n",
    "    text_hash = [word_to_hash.get(w.lower(),0) for w in text_split]\n",
    "    text_pad = sequence.pad_sequences([text_hash], maxlen=50)\n",
    "    return text_pad\n",
    "\n",
    "label_pos_dict = {0:'NN',\n",
    "                  1:'NNP',\n",
    "                  2:'IN',\n",
    "                  3:'DT',\n",
    "                  4:'JJ',\n",
    "                  5:'NNS',\n",
    "                  6:'PUNCT',\n",
    "                  7:'VBD',\n",
    "                  8:'VBN',\n",
    "                  9:'OTHER'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0, 12343, 14984,\n",
       "        28149, 26507, 12343, 10504, 10663]], dtype=int32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_text_to_sequence(\"The dog ran across the street.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(model.predict(plain_text_to_sequence(\"The dog ran across the street.\"))[0,:,:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[label_pos_dict.get(x,\"NONE\") for x in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
